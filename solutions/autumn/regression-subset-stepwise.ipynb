{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression: Best Subset and Stepwise Regression\n",
    "\n",
    "**Functions**\n",
    "\n",
    "`np.linalg.lstsq`, `sklearn.model_selection.cross_val_score`, `sklearn.linear_model.LinearRegression`, `sklearn.model_selection.KFold`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 41\n",
    "Download data from [Ken French's website](https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/) on the VWM and the 12 industry portfolios. Reformat both data sets so that they has a `DatetimeIndex`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kevin\\AppData\\Local\\Temp\\ipykernel_68192\\1717907297.py:3: FutureWarning: 'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n",
      "  vwm = vwm.resample(\"M\").last()\n",
      "C:\\Users\\kevin\\AppData\\Local\\Temp\\ipykernel_68192\\1717907297.py:7: FutureWarning: 'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n",
      "  industries = industries.resample(\"M\").last()\n"
     ]
    }
   ],
   "source": [
    "vwm = pd.read_csv(\"data/VWM.csv\", index_col=\"Date\")\n",
    "vwm.index = pd.to_datetime(vwm.index, format=\"%Y%m\")\n",
    "vwm = vwm.resample(\"M\").last()\n",
    "\n",
    "industries = pd.read_csv(\"data/12_Industry_portfolios.csv\", index_col=\"Date\")\n",
    "industries.index = pd.to_datetime(industries.index, format=\"%Y%m\")\n",
    "industries = industries.resample(\"M\").last()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 42\n",
    "\n",
    "Use Best Subset Regression with cross-validation to select the weights of a tracking portfolio where the industry portfolios are used to track the value-weighted-market. Use data until the end of 2014. A tracking portfolio is a portfolio that replicates a portfolio using other assets. The weights can be estimated using a regression model excludes a constant.\n",
    "\n",
    "$$ R_{i,p} = \\beta_1 R_{i,1} + \\beta_2 R_{i,2} + \\ldots + \\beta_k R_{i,k} + \\epsilon_{i} $$\n",
    "\n",
    "where $R_{i,j}$ are returns on the assets used to construct the tracking portfolio. The constant is excluded since the portfolio should track both the shock and match the mean return. OLS minimizes the variance of the tracking error (in-sample)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = industries[\"1980\":\"2014\"]\n",
    "y = vwm[\"VWM\"][\"1980\":\"2014\"]\n",
    "t, p = x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation\n",
    "\n",
    "We start by downloading the data which have all been downloaded from [Ken French's website](https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/data_library.html).  In this exercise we will use the Value-weighted Market Return and 12 industry portfolios.\n",
    "\n",
    "The data is subsetted into 1980 - 2014 (inclusive) and the data from 2015 to the end of the sample will be used to evaluate model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('NoDur',)\n",
      "('Durbl',)\n",
      "('NoDur', 'Durbl')\n",
      "('NoDur', 'Manuf')\n",
      "('NoDur', 'Durbl', 'Manuf')\n",
      "('NoDur', 'Durbl', 'Enrgy')\n",
      "('NoDur', 'Durbl', 'Manuf', 'Enrgy')\n",
      "('NoDur', 'Durbl', 'Manuf', 'Chems')\n",
      "('NoDur', 'Durbl', 'Manuf', 'Enrgy', 'Chems')\n",
      "('NoDur', 'Durbl', 'Manuf', 'Enrgy', 'BusEq')\n",
      "('NoDur', 'Durbl', 'Manuf', 'Enrgy', 'Chems', 'BusEq')\n",
      "('NoDur', 'Durbl', 'Manuf', 'Enrgy', 'Chems', 'Telcm')\n",
      "('NoDur', 'Durbl', 'Manuf', 'Enrgy', 'Chems', 'BusEq', 'Telcm')\n",
      "('NoDur', 'Durbl', 'Manuf', 'Enrgy', 'Chems', 'BusEq', 'Utils')\n",
      "('NoDur', 'Durbl', 'Manuf', 'Enrgy', 'Chems', 'BusEq', 'Telcm', 'Utils')\n",
      "('NoDur', 'Durbl', 'Manuf', 'Enrgy', 'Chems', 'BusEq', 'Telcm', 'Shops')\n",
      "('NoDur', 'Durbl', 'Manuf', 'Enrgy', 'Chems', 'BusEq', 'Telcm', 'Utils', 'Shops')\n",
      "('NoDur', 'Durbl', 'Manuf', 'Enrgy', 'Chems', 'BusEq', 'Telcm', 'Utils', 'Hlth ')\n",
      "('NoDur', 'Durbl', 'Manuf', 'Enrgy', 'Chems', 'BusEq', 'Telcm', 'Utils', 'Shops', 'Hlth ')\n",
      "('NoDur', 'Durbl', 'Manuf', 'Enrgy', 'Chems', 'BusEq', 'Telcm', 'Utils', 'Shops', 'Money')\n",
      "('NoDur', 'Durbl', 'Manuf', 'Enrgy', 'Chems', 'BusEq', 'Telcm', 'Utils', 'Shops', 'Hlth ', 'Money')\n",
      "('NoDur', 'Durbl', 'Manuf', 'Enrgy', 'Chems', 'BusEq', 'Telcm', 'Utils', 'Shops', 'Hlth ', 'Other')\n",
      "('NoDur', 'Durbl', 'Manuf', 'Enrgy', 'Chems', 'BusEq', 'Telcm', 'Utils', 'Shops', 'Hlth ', 'Money', 'Other')\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "for i in range(1, p + 1):\n",
    "    count = 0\n",
    "    for comb in combinations(x.columns, i):\n",
    "        print(comb)\n",
    "        count += 1\n",
    "        if count > 1:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation\n",
    "\n",
    "We can use `combinations` from `itertools` to construct all combinations of `k` elemments from a list with `p`.  Looping from `k=1` to `p` allows all distinct combinations to be constructed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1                                               [Other]\n",
       "2                                        [BusEq, Money]\n",
       "3                                 [NoDur, BusEq, Other]\n",
       "4                          [NoDur, Enrgy, BusEq, Money]\n",
       "5                   [NoDur, Enrgy, BusEq, Money, Other]\n",
       "6            [NoDur, Enrgy, BusEq, Telcm, Money, Other]\n",
       "7     [NoDur, Enrgy, BusEq, Telcm, Hlth , Money, Other]\n",
       "8     [NoDur, Manuf, Enrgy, BusEq, Telcm, Hlth , Mon...\n",
       "9     [NoDur, Manuf, Enrgy, BusEq, Telcm, Shops, Hlt...\n",
       "10    [NoDur, Manuf, Enrgy, BusEq, Telcm, Utils, Sho...\n",
       "11    [NoDur, Manuf, Enrgy, Chems, BusEq, Telcm, Uti...\n",
       "12    [NoDur, Durbl, Manuf, Enrgy, Chems, BusEq, Tel...\n",
       "dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy.linalg import lstsq\n",
    "\n",
    "best_models = {}\n",
    "for i in range(1, p + 1):\n",
    "    best_sse = np.inf\n",
    "    count = 0\n",
    "    for comb in combinations(x.columns, i):\n",
    "        if count > 1:\n",
    "            break\n",
    "        reg = x[list(comb)]\n",
    "        beta = lstsq(reg, y, rcond=None)[0]\n",
    "        resid = y - reg @ beta\n",
    "        sse = resid @ resid\n",
    "        if sse < best_sse:\n",
    "            best_sse = sse\n",
    "            best_models[i] = list(comb)\n",
    "pd.Series(best_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation\n",
    "\n",
    "Best subset regression involves fitting all possible model.  Python's `itertools` module has a function `combinations` that will construct all distinct combinations of $n$ items from a collection. This can be used with different $n$ to build all possible models. Below we see that it outputs list of columns names.\n",
    "\n",
    "We use `lstsq` from `numpy.linalg` to estimate the regression parameters.  This function is quite a bit faster than `OLS` from `statsmodels. It returns multiple outputs but we only need the first which is selected using `[0]`.\n",
    "\n",
    "The best model is selected by iterating across all model sizes and retaining the model with the smallest SSE. The variable `best_sse` is initialized to be infinity so that at least one model will always be smaller.\n",
    "\n",
    "Finally, we store the columns of the best model in a dictionary where the key is the number of variables in the model. The last line prints the set of models selected.\n",
    "\n",
    "**Caution**\n",
    "\n",
    "The number of distinct models grows at rate $2^p$ and so this code may be very slow when $p$ is modestly large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(771.9822535009077)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def xval_5fold(y, x, random=False, seed=20201231):\n",
    "    # Use numpy arrays for simplicity\n",
    "    y = np.asarray(y)\n",
    "    x = np.asarray(x)\n",
    "    n = y.shape[0]\n",
    "    if random:\n",
    "        # If randomization is needed, use the seed\n",
    "        rg = np.random.default_rng(seed)\n",
    "        # Generate a set of index values to use to randomly reorder the data\n",
    "        # After randomization, we can use the data as if it is inorder\n",
    "        ind = rg.permutation(np.arange(n))\n",
    "        y = y[ind]\n",
    "        x = x[ind]\n",
    "    # Compute the block size\n",
    "    block = n / 5.0\n",
    "    sse = 0.0\n",
    "    for i in range(5):\n",
    "        # Start and End of each block need to be integers\n",
    "        # Rounding ensures that we get all observations since int rounds down\n",
    "        st = int(np.round(i * block))\n",
    "        en = int(np.round((i + 1) * block))\n",
    "        # Constrct the indicaes of the observations that we leave out\n",
    "        leave_out = np.r_[st:en]\n",
    "        # The included are those that we don't leave out\n",
    "        include = np.setdiff1d(np.arange(n), leave_out)\n",
    "        # Compute the regression coefficients\n",
    "        beta = lstsq(x[include], y[include], rcond=None)[0]\n",
    "        # Compute the residuals and sdd to the SSE\n",
    "        resid = y[st:en] - x[st:en] @ beta\n",
    "        sse += resid @ resid\n",
    "    return sse\n",
    "\n",
    "\n",
    "xval_5fold(y, x[best_models[2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation\n",
    "\n",
    "We need to cross-validate the set of best models. To do this, we can write a function (see the method available through scikit-learn that will simplify this step) so that we can reuse it later. The function defaults to non-random cross-validation which will leave out blocks containing 20% of the data in order. The function can optionally be used to randomly select the data left out.\n",
    "\n",
    "When we randomize we get different values.  Randomization may select different models.  It usually makes little difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(750.3129727221956)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xval_5fold(y, x[best_models[2]], random=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation \n",
    "\n",
    "We can apply `xval_5fold` to the data to compute the cross-validate SSE, and retain the best. We loop over the keys in the `best_models` and select the contents of the dictionary using `best_models[n_var]`.\n",
    "\n",
    "While some of the cross-validated SSEs are very large, many are similar to the best. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The minimum k is 9\n",
      "The selected model uses ['NoDur', 'Manuf', 'Enrgy', 'BusEq', 'Telcm', 'Shops', 'Hlth ', 'Money', 'Other']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1     1148.876144\n",
       "2      771.982254\n",
       "3      494.175842\n",
       "4      368.090926\n",
       "5      283.350797\n",
       "6      235.289796\n",
       "7      210.449663\n",
       "8      208.251414\n",
       "9      204.369481\n",
       "10     208.891542\n",
       "11     209.923549\n",
       "12     209.984916\n",
       "dtype: float64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bsr_sse_xv = {}\n",
    "for n_var in best_models:\n",
    "    bsr_sse_xv[n_var] = xval_5fold(y, x[best_models[n_var]])\n",
    "bsr_sse_xv = pd.Series(bsr_sse_xv)\n",
    "\n",
    "print(f\"The minimum k is {bsr_sse_xv.idxmin()}\")\n",
    "print(f\"The selected model uses {best_models[bsr_sse_xv.idxmin()]}\")\n",
    "bsr_model = best_models[bsr_sse_xv.idxmin()]\n",
    "bsr_sse_xv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation\n",
    "\n",
    "If we use randomization we may select a different model. The alternative model will usually have an SSE that is close to the SSE of the previous model selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The minimum k is 10\n",
      "The selected model uses ['NoDur', 'Manuf', 'Enrgy', 'BusEq', 'Telcm', 'Utils', 'Shops', 'Hlth ', 'Money', 'Other']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1     1142.713474\n",
       "2      750.312973\n",
       "3      469.750212\n",
       "4      334.086464\n",
       "5      246.304735\n",
       "6      197.971097\n",
       "7      175.919440\n",
       "8      161.668160\n",
       "9      157.701899\n",
       "10     156.962597\n",
       "11     157.765810\n",
       "12     158.046894\n",
       "dtype: float64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sse_xv = {}\n",
    "for k in best_models:\n",
    "    sse_xv[k] = xval_5fold(y, x[best_models[k]], random=True)\n",
    "sse_xv = pd.Series(sse_xv)\n",
    "print(f\"The minimum k is {sse_xv.idxmin()}\")\n",
    "print(f\"The selected model uses {best_models[sse_xv.idxmin()]}\")\n",
    "sse_xv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 43\n",
    "\n",
    "Select the best tracking portfolio using Forward Stepwise Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation\n",
    "\n",
    "Forward Stepwise Regression is implemented by tracking the variables that have been included in the model. We then add one variables from those that are excluded to the included and find the model that minimizes the SSE from the next iteration. The is repeated until all variables are added.  The list of included contains the variables in the order they are added, so that `included[:k]` returns the list of included variables in a model containing `k` variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Other', 'BusEq', 'NoDur', 'Money', 'Enrgy', 'Telcm', 'Hlth ', 'Manuf', 'Shops', 'Utils', 'Chems', 'Durbl']\n"
     ]
    }
   ],
   "source": [
    "# Initialize the list of included which will store variables in order\n",
    "included = []\n",
    "\n",
    "for i in range(p):\n",
    "    # Excluded are the columns that are not in included\n",
    "    excluded = [col for col in x if col not in included]\n",
    "    # Initialize to inf so that it will be beated by some model\n",
    "    best_sse = np.inf\n",
    "    # Loop across the excludded\n",
    "    for col in excluded:\n",
    "        # Add the new column to the included list and select the x data\n",
    "        try_x = x[included + [col]]\n",
    "        # Compute the regression and the SSE\n",
    "        beta = lstsq(try_x, y, rcond=None)[0]\n",
    "        resid = y - try_x @ beta\n",
    "        sse = resid @ resid\n",
    "        # If this sse is better than the best seen, keep it and retain the new variable added\n",
    "        if sse < best_sse:\n",
    "            best_sse = sse\n",
    "            next_var = col\n",
    "    # Add the variable that reduced the SSE the most\n",
    "    included.append(next_var)\n",
    "\n",
    "print(included)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation \n",
    "\n",
    "Next we cross-validate the FSR sequence of models by looping across the list of included variables, in order.\n",
    "\n",
    "We store the cross-validated SSE as a dictionary that we can convert to a `Series`. We can then get the best using `idxmin()` to return the index associated with the smallest cross-validated SSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The minimum k is 9\n",
      "The selected variables are ['Other', 'BusEq', 'NoDur', 'Money', 'Enrgy', 'Telcm', 'Hlth ', 'Manuf', 'Shops']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1     1148.876144\n",
       "2      785.510416\n",
       "3      494.175842\n",
       "4      373.038967\n",
       "5      283.350797\n",
       "6      235.289796\n",
       "7      210.449663\n",
       "8      208.251414\n",
       "9      204.369481\n",
       "10     208.891542\n",
       "11     209.923549\n",
       "12     209.984916\n",
       "dtype: float64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fsr_sse_sv = {}\n",
    "for i in range(1, p + 1):\n",
    "    fsr_sse_sv[i] = xval_5fold(y, x[included[:i]])\n",
    "fsr_sse_sv = pd.Series(fsr_sse_sv)\n",
    "print(f\"The minimum k is {fsr_sse_sv.idxmin()}\")\n",
    "forstep_model = included[: fsr_sse_sv.idxmin()]\n",
    "print(f\"The selected variables are {forstep_model}\")\n",
    "fsr_sse_sv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 44\n",
    "\n",
    "Use Backward Stepwise Regression to select the tracking portfolio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation\n",
    "\n",
    "Backward Stepwise Regression is substantially similar to Forward except that we being with the included set contains all columns and then remove one at a time.  We always retain the model with the smallest SSE that has one variable removed from the previous model.\n",
    "\n",
    "We can then cross-validate the BSR models using the same code as for FSR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Durbl', 'Chems', 'Utils', 'Shops', 'Manuf', 'Hlth ', 'Telcm', 'Other', 'Enrgy', 'NoDur', 'BusEq', 'Money']\n"
     ]
    }
   ],
   "source": [
    "# Initialize the set of included to all columns\n",
    "included = list(x.columns)\n",
    "# Initialize the list of removed variables\n",
    "removed = []\n",
    "\n",
    "for i in range(p):\n",
    "    # Ensure the at least one model will beat the best SSE\n",
    "    best_sse = np.inf\n",
    "    # Iteratove over columns that are in the model\n",
    "    for col in included:\n",
    "        # Copy the included list\n",
    "        try_col = included[:]\n",
    "        # and remove col from the list\n",
    "        try_col.remove(col)\n",
    "        # Get the set of x data for these columns\n",
    "        try_x = x[try_col]\n",
    "        # Compute the coefficients and sse\n",
    "        beta = lstsq(try_x, y, rcond=None)[0]\n",
    "        resid = y - try_x @ beta\n",
    "        sse = resid @ resid\n",
    "        # If the sse is less than the best seen so far, retain it\n",
    "        if sse < best_sse:\n",
    "            # Retain the sse and update the next to drop variable\n",
    "            best_sse = sse\n",
    "            next_drop = col\n",
    "    # Add the next variable to drop\n",
    "    removed.append(next_drop)\n",
    "    # Remove the variable that we drop so that it doesn't show up in the next round\n",
    "    included.remove(next_drop)\n",
    "\n",
    "# The list of variables dropped, in order\n",
    "print(removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The minimum k is 9\n",
      "The selected variables are ['Money', 'BusEq', 'NoDur', 'Enrgy', 'Other', 'Telcm', 'Hlth ', 'Manuf', 'Shops']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1     2348.988056\n",
       "2      771.982254\n",
       "3      528.393322\n",
       "4      368.090926\n",
       "5      283.350797\n",
       "6      235.289796\n",
       "7      210.449663\n",
       "8      208.251414\n",
       "9      204.369481\n",
       "10     208.891542\n",
       "11     209.923549\n",
       "12     209.984916\n",
       "dtype: float64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reversing the dropped list gets us the list of variables included fo\n",
    "# any number of regressors\n",
    "included = removed[::-1]\n",
    "\n",
    "# This is identical to the FSR cross-validation code\n",
    "bsr_sse_sv = {}\n",
    "for i in range(1, p + 1):\n",
    "    bsr_sse_sv[i] = xval_5fold(y, x[included[:i]])\n",
    "bsr_sse_sv = pd.Series(bsr_sse_sv)\n",
    "print(f\"The minimum k is {bsr_sse_sv.idxmin()}\")\n",
    "backstep_model = included[: bsr_sse_sv.idxmin()]\n",
    "print(f\"The selected variables are {backstep_model}\")\n",
    "bsr_sse_sv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 45\n",
    "\n",
    "Using scikit-learn to cross-validate the Backward Stepwise selected models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation\n",
    "\n",
    "scikit-learn is a machine-learning library that can contains codes for many models and tasks. This includes OLS and cross-validation. Below we show how the function `cross_val_score` can be used to cross-validate a set of linear regression models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum k is 9\n",
      "The selected variables are ['Money', 'BusEq', 'NoDur', 'Enrgy', 'Other', 'Telcm', 'Hlth ', 'Manuf', 'Shops']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1    -5.592829\n",
       "2    -1.838053\n",
       "3    -1.258079\n",
       "4    -0.876407\n",
       "5    -0.674645\n",
       "6    -0.560214\n",
       "7    -0.501071\n",
       "8    -0.495837\n",
       "9    -0.486594\n",
       "10   -0.497361\n",
       "11   -0.499818\n",
       "12   -0.499964\n",
       "dtype: float64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import from sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "# Initialize a LR that will not include an intercept\n",
    "lr = LinearRegression(fit_intercept=False)\n",
    "\n",
    "# A dictionary to hold the MSE\n",
    "bsr_mse_cv = {}\n",
    "for i in range(1, p + 1):\n",
    "    # Get the x data to use\n",
    "    included_x = x[included[:i]]\n",
    "    # Call cross_val_score with the model, the and and y data, and ask it so use\n",
    "    # neg_mean_squared_error to cross-validate.  We will see below that\n",
    "    # -t * neg_mean_squared_error is the SSE, so the maximum neg_mean_squared_error\n",
    "    # will select the same model as the minimum SSE.\n",
    "    # We need to take the mean since it reports the MSE for each fold of the data (5 by default)\n",
    "    bsr_mse_cv[i] = cross_val_score(\n",
    "        lr, included_x, y, scoring=\"neg_mean_squared_error\"\n",
    "    ).mean()\n",
    "\n",
    "bsr_mse_cv = pd.Series(bsr_mse_cv)\n",
    "# We use the maximum now\n",
    "print(f\"The maximum k is {bsr_mse_cv.idxmax()}\")\n",
    "# The variables are the same\n",
    "print(f\"The selected variables are {included[:bsr_mse_cv.idxmax()]}\")\n",
    "bsr_mse_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation\n",
    "\n",
    "The negative MSE is simply \n",
    "\n",
    "$$ \\text{Neg. MSE} = -\\frac{SSE}{t} $$\n",
    "\n",
    "so it is simple to invert back to SSE. These values are the same as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1     2348.988056\n",
       "2      771.982254\n",
       "3      528.393322\n",
       "4      368.090926\n",
       "5      283.350797\n",
       "6      235.289796\n",
       "7      210.449663\n",
       "8      208.251414\n",
       "9      204.369481\n",
       "10     208.891542\n",
       "11     209.923549\n",
       "12     209.984916\n",
       "dtype: float64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert to SSE\n",
    "alt_sse = -t * bsr_mse_cv\n",
    "alt_sse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 46\n",
    "\n",
    "Repeat the cross-validation using scikit-learn using randomly selected values in each block."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation\n",
    "\n",
    "The default cross-validation blocks the data in the order it occurs in the inputs. If randomization is desired it is necessary to pass a cross-validation scheme that will randomize.  We do these here using `KFold` with `shuffle=True`. We also pass a `random_state` seed value to ensure that the cross-validation is reproducible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum k is 11\n",
      "The selected variables are ['Money', 'BusEq', 'NoDur', 'Enrgy', 'Other', 'Telcm', 'Hlth ', 'Manuf', 'Shops', 'Utils', 'Chems']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1    -5.436108\n",
       "2    -1.799260\n",
       "3    -1.193152\n",
       "4    -0.759860\n",
       "5    -0.574028\n",
       "6    -0.461058\n",
       "7    -0.406622\n",
       "8    -0.379140\n",
       "9    -0.369990\n",
       "10   -0.371283\n",
       "11   -0.369444\n",
       "12   -0.380203\n",
       "dtype: float64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "lr = LinearRegression(fit_intercept=False)\n",
    "\n",
    "bsr_mse_cv = {}\n",
    "cv = KFold(5, shuffle=True, random_state=20201231)\n",
    "\n",
    "for i in range(1, p + 1):\n",
    "    bsr_mse_cv[i] = cross_val_score(\n",
    "        lr, x[included[:i]], y, scoring=\"neg_mean_squared_error\", cv=cv\n",
    "    ).mean()\n",
    "bsr_mse_cv = pd.Series(bsr_mse_cv)\n",
    "print(f\"The maximum k is {bsr_mse_cv.idxmax()}\")\n",
    "print(f\"The selected variables are {included[:bsr_mse_cv.idxmax()]}\")\n",
    "bsr_mse_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 47\n",
    "\n",
    "Evaluate the models selected using the sample that was _not_ used in fitting to assess the out-of-sample performance based on SSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation\n",
    "\n",
    "Finally, we can look at out-of-sample evaluation.  We simply estimate the regression coefficients in-sample and then use these with the out-of-sample x values. The residuals are the computed from these out-of-sample prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Subset OOS SSE: 22.398513710136537\n"
     ]
    }
   ],
   "source": [
    "y_oos = vwm.loc[\"2015\":, \"VWM\"]\n",
    "x_oos = industries[\"2015\":]\n",
    "beta = lstsq(x[bsr_model], y, rcond=None)[0]\n",
    "pred = x_oos[bsr_model] @ beta\n",
    "resid_oos = y_oos - x_oos[bsr_model] @ beta\n",
    "oos_sse = resid_oos @ resid_oos\n",
    "print(f\"Best Subset OOS SSE: {oos_sse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward Stepwise OOS SSE: 22.39851371013647\n"
     ]
    }
   ],
   "source": [
    "beta = lstsq(x[forstep_model], y, rcond=None)[0]\n",
    "pred = x_oos[forstep_model] @ beta\n",
    "resid_oos = y_oos - x_oos[forstep_model] @ beta\n",
    "oos_sse = resid_oos @ resid_oos\n",
    "print(f\"Forward Stepwise OOS SSE: {oos_sse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backward Stepwise OOS SSE: 22.398513710136537\n"
     ]
    }
   ],
   "source": [
    "beta = lstsq(x[backstep_model], y, rcond=None)[0]\n",
    "pred = x_oos[backstep_model] @ beta\n",
    "resid_oos = y_oos - x_oos[backstep_model] @ beta\n",
    "oos_sse = resid_oos @ resid_oos\n",
    "print(f\"Backward Stepwise OOS SSE: {oos_sse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 47\n",
    "\n",
    "Compute the in-sample and out-of-sample $R^2$ for the selected models.\n",
    "\n",
    "The out-of-sample $R^2$ defined as\n",
    "\n",
    "$$ 1 - \\frac{SSE_{OOS}}{TSS_{OOS}} $$\n",
    "\n",
    "where the $SSE_{OOS}$ is the SSE using the predicted values and the $TSS_{OOS}$ is the TSS computed using the in-sample value (without demeaning since the models we are fitting do not include a constant). **Note**: If a model does not contain a constant, the $TSS_{OOS}$ is _not_ demeaned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The out-of-sample R2 for the out-of-sample period is 98.4%.\n"
     ]
    }
   ],
   "source": [
    "oos_tss = (y_oos**2).sum()\n",
    "oos_r2 = 1 - oos_sse / oos_tss\n",
    "print(f\"The out-of-sample R2 for the out-of-sample period is {100*oos_r2:0.1f}%.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The in-sample R2 for the out-of-sample period is 99.9%.\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.api import OLS\n",
    "\n",
    "r2 = OLS(y_oos, x_oos).fit().rsquared\n",
    "print(f\"The in-sample R2 for the out-of-sample period is {100*r2:0.1f}%.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 48\n",
    "\n",
    "Use scikit-learn to produce out-of-sample fits and compute the out-of-sample SSE.  Verify this value is the same as you found previously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation\n",
    "\n",
    "scikit-learn is optimized for prediction and so producing out-of-sample prediction is particularly simple using the `predict` method of a fitted model. The strategy with scikit-learn models is always the same:\n",
    "\n",
    "1. Initialize the model instance and set any configuration options.\n",
    "2. Fit the model using the `fit(x, y)` method using the in-sample data\n",
    "3. Predict the out-of-sample values used the `predict(x)` method using the same model instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(22.398513710136537)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specify and fit the model\n",
    "lr = LinearRegression(fit_intercept=False).fit(x[bsr_model], y)\n",
    "# Predictions use the coefficient estimated previously and the new data\n",
    "pred = lr.predict(x_oos[bsr_model])\n",
    "resid_oos = y_oos - pred\n",
    "oos_sse = resid_oos @ resid_oos\n",
    "oos_sse"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
